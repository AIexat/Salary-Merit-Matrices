# Бюджетирование повышений зарплат: Валидация метода, Анализ ограничений и Оптимизаци (Merit Matrix Budgeting: Validation, Analysis, and Optimization)

## Автор
Алексей Иванов  
Итоговый проект программы «Специалист по Data Science», НИУ ВШЭ

---

## Содержание
1. [Краткое резюме](#краткое-резюме)
2. [Описание проблемы](#описание-проблемы)
3. [Структура проекта](#структура-проекта)
4. [Часть 1: Monte Carlo Validation](#часть-1-monte-carlo-validation)
5. [Часть 2: Анализ результатов](#часть-2-анализ-результатов)
6. [Часть 3: Генетический алгоритм - решение проблемы](#часть-3-генетический-алгоритм---решение-проблемы)
7. [Общие выводы](#общие-выводы)

---

## Краткое резюме

### TL;DR

**Исследуемый вопрос:** Насколько надежен широко используемый метод агрегированных распределений для расчета бюджета повышений зарплат?

**Метод:** Monte Carlo симуляция на данных 395,325 сотрудников из Москвы, тестирование 120 конфигураций merit matrices.

**Главный вывод:** Метод имеет фундаментальное ограничение применимости — требует 200+ сотрудников для приемлемой точности (80%+ success rate), что делает его **непригодным для 75-80% компаний** из-за необходимости функциональной сегментации.

**Решение:** Разработан генетический алгоритм, который преодолевает выявленные ограничения и работает для групп от 50 человек с success rate 82-94%.

---

## Описание проблемы

### Что такое Merit Matrix?

**Merit Matrix (матрица повышений)** — инструмент HR для определения процента повышения зарплаты на основе двух параметров:

1. **Compa Ratio (CR)** — отношение текущей зарплаты к рыночной медиане или серединной точки в структуре вознаграждения (midpoint) для сотрудника:
   - CR < 1.0: ниже рынка
   - CR = 1.0: на рынке
   - CR > 1.0: выше рынка

2. **Performance Rating** — оценка эффективности сотрудника, определяемая непосредственным руковожителем (обычно от 3 до 7)

**Пример Merit Matrix 5×5:**

| CR / Rating | R1 | R2 | R3 | R4 | R5 |
|-------------|----|----|----|----|----| 
| < 0.80 | 6% | 10% | 14% | 17% | 20% |
| 0.80-0.90 | 5% | 9% | 12% | 15% | 18% |
| 0.90-1.10 | 3% | 7% | 10% | 13% | 16% |
| 1.10-1.20 | 2% | 5% | 8% | 11% | 13% |
| > 1.20 | 1% | 3% | 6% | 8% | 10% |

### Фундаментальная проблема прогнозирования бюджета

**Проблема временного разрыва:**
- **Q3-Q4:** HR утверждает бюджет на повышения
- **Q1:** Менеджеры присваивают рейтинги сотрудникам
- **Разрыв:** 3-6 месяцев между планированием и реализацией

**Что HR знает:**
- Выделенный бюджет на повышение заработных плат для группы сотрудников
- Merit Matrix (проценты повышений), которая разрабатывается самим HR
- Распределение сотрудников по диапазоном CR
- Целевое распределение рейтингов (10%-20%-40%-20%-10%), которое задается политикой или историческими данными

**Что HR не знает и не контролирует:**
- Какому конкретно сотруднику (с каким CR) какой рейтинг будет присвоен
- Будут ли менеджеры строго придерживаться заданного распределения

### Метод агрегированных распределений

HR использует формулу:

```
Ожидаемый бюджет = Σ P(CR_bin_i) × P(Rating_j) × Merit(i,j)
```

где:
- `P(CR_bin_i)` — доля сотрудников в CR-бине (известно из данных)
- `P(Rating_j)` — целевая доля рейтинга j (задается политикой)
- `Merit(i,j)` — процент из матрицы

**Почему "агрегированных распределений"?**
- Метод **агрегирует** (объединяет) два независимых распределения:
  1. Распределение сотрудников по CR bins (известно из данных)
  2. Целевое распределение рейтингов (задается HR политикой)

- Бюджет рассчитывается как математическое ожидание по этим распределениям

**Вопрос исследования:** Насколько точен этот метод в реальности? Как часто реальный бюджет попадает в заданный диапазон (например, ±5% от расчета по агрегированным распределениям)?
---

## Структура проекта
```
merit_matrix_project/
├── data/
│   └── Moscow.parquet.gzip                    # Подготовленные данные
├── notebooks/
│   ├── 01_Monte_Carlo_Simulation.ipynb        # Часть 1: Симуляции
│   ├── 02_Results_Analysis.ipynb              # Часть 2: Анализ
│   └── 03_Genetic_Algorithm_Optimizer.ipynb   # Часть 3: Оптимизация
├── results/
│   └── merit_analysis_results/                # Результаты симуляций
├── visualizations/
│   └── success_rate_by_parameters.png         # Графики
└── README.md                                  # Данный документ
```


## Часть 1: Monte Carlo Validation

### Цель

Проверить точность метода агрегированных распределений через симуляцию реальных сценариев присвоения рейтингов.

### Методология

**Для каждой группы сотрудников (Company × Function):**

1. **Рассчитать baseline:** бюджет по методу агрегированных распределений
2. **Провести 50,000 симуляций:**
   - Случайно присвоить рейтинги согласно целевому распределению
   - Добавить реалистичные вариации (Dirichlet distribution)
   - Рассчитать реальный бюджет по merit matrix
   - Проверить попадание в диапазон ±5%
3. **Рассчитать success rate:** % симуляций в пределах допуска

### Тестирование робастности

Для проверки устойчивости метода анализ проведен на **120 различных конфигурациях**:
- 6 вариантов CR bins (2-7)
- 5 вариантов ratings (3-7)
- 4 типа распределений (normal, skewed_high, skewed_low, uniform)

**Цель:** Убедиться, что результаты не зависят от конкретного дизайна матрицы.

### Данные

**Источник:** Исследование по вознаграждению Lens Consulting c референтной датой 1 мая 2025
- **Регион:** Москва
- **Сотрудников:** 383 483 сотрудника
- **Группировка:** Company × Function (2921 групп)
- **Размеры групп:** от 10 до 5000+ сотрудников

### Технические решения

#### Используемые библиотеки

**Обработка данных:**
- `pandas` — группировка по Company×Function, расчет медиан
- `numpy` — векторизованные операции с массивами

**Статистическое моделирование:**
- `scipy.stats.dirichlet` — моделирование вариаций распределения рейтингов
  - Параметр `concentration=250` создает отклонения ±2-3% от целевого распределения
  - Имитирует реальность: менеджеры следуют политике, но с небольшими исключениями

**Оптимизация производительности:**
- `numba` — JIT-компиляция критического цикла Monte Carlo
  - Ускорение в ~100 раз vs чистый Python
  - Параллелизация на все CPU cores (`@jit(parallel=True)`)
  - **Результат:** ~2-3 секунды на группу из 100 сотрудников (50K итераций)

#### Ключевые технические решения

**1. Подготовка данных**

```python
# Расчет Midpoint (медиана зарплат по Company×Function×Grade)
midpoints = df.groupby(['Company', 'Function', 'Grade'])['BP'].median()

# Расчет Compa Ratio для каждого сотрудника
CR = Base_Pay / Midpoint

# Агрегация: убираем Grade (CR уже учитывает позицию относительно грейда)
```

**Обоснование:** Использование медианы для устойчивости к выбросам; стандартная практика в compensation management.

**2. Генерация Merit Matrices**

Автоматическая генерация матриц с соблюдением принципов справедливости:
- Высокий рейтинг → больше повышение (70% влияния)
- Высокий CR → меньше повышение (30% влияния)
- Диапазон: 1% (worst case) до 20% (best case)

**3. Оптимизированный Monte Carlo**

**Векторизация:** Все случайные числа генерируются до основного цикла
```python
# Pre-generation для 50,000 итераций
all_sampled_probs = dirichlet.rvs(alpha, size=50000)  # Dirichlet samples
all_random_values = np.random.random((50000, n_employees))  # Uniform RV
```

**Numba JIT-компиляция:**
```python
@jit(nopython=True, parallel=True)
def _run_simulations_weighted(...):
    for sim_idx in prange(n_simulations):  # Параллельный цикл
        # Присвоение рейтингов
        # Расчет merit amounts
        # Проверка попадания в tolerance
```

**4. Класс FlexibleMeritAnalyzer**

Универсальный анализатор для матриц любого размера (2×3 до 7×7):
- `calculate_heuristic_budget()` — реализация метода агрегированных распределений
- `run_monte_carlo_for_group()` — 50,000 симуляций для одной группы
- `process_dataframe()` — обработка всех групп Company×Function с автофильтрацией малых выборок

#### Воспроизводимость

**Сохранение результатов:**
- Формат: Parquet (компактный, быстрый)
- Naming: `merit_analysis_{config_name}_{timestamp}.parquet`
- Метаданные: config_name, n_cr_bins, n_ratings, distribution_type

**Время выполнения:**
- Одна конфигурация (~300 групп): 10-15 минут
- Все 120 конфигураций: 2-4 часа на стандартном laptop (8 cores, 16GB RAM)

---

## Часть 2: Анализ результатов

![Threshold Analysis](threshold_analysis.png)

### Ключевая находка: критическая зависимость от размера группы

**Success rate по размеру группы:**

| Размер группы | Success Rate | Интерпретация |
|---------------|--------------|---------------|
| 0-20 | ~33% | ❌ Катастрофа: 2 из 3 выход за бюджет |
| 21-50 | ~46% | ❌ Провал: >50% выход за бюджет |
| 51-100 | ~61% | ❌ Неприемлемо: 40% выход за бюджет |
| 101-200 | ~77% | ⚠️ Рискованно: 23% выход за бюджет |
| 201-500 | ~91% | ✅ Приемлемо: 9% выход за бюджет |
| 500+ | ~98% | ✅ Надежно: 2% выход за бюджет |

**Критический порог:** 200+ сотрудников для достижения 80%+ success rate.

**Причина:** Закон больших чисел
- Малые группы: высокая вариативность → непредсказуемый бюджет
- Большие группы: распределение стремится к ожидаемому → предсказуемый бюджет

### Влияние других факторов (минимальное)

**Конфигурация матрицы:**
- Количество CR bins (2-7): разброс success rate 47-53% (±6%)
- Количество Ratings (3-7): разброс 48-54% (±6%)
- **Вывод:** Размер матрицы практически не влияет на точность

**Тип распределения рейтингов:**
- Разброс: 43-57% (14 процентных пунктов)
- **Вывод:** Умеренное влияние, значительно меньше размера группы

**Сводная таблица влияния:**

| Фактор | Разброс Success Rate | Влияние |
|--------|---------------------|---------|
| **Размер группы** | **33% → 98%** (65%) | ⭐⭐⭐⭐⭐ Критический |
| Тип распределения | 43% → 57% (14%) | ⭐⭐ Умеренное |
| Количество CR bins | 47% → 53% (6%) | ⭐ Минимальное |
| Количество Ratings | 48% → 54% (6%) | ⭐ Минимальное |

**Вывод:** Размер группы объясняет ~85-90% вариации в точности.

### Парадокс практической применимости

**Реальность бизнеса:** Компании **не могут** применять единую матрицу ко всей организации:

**Пример: компания 1,000 сотрудников**

```
Сегментация по функциям:
├── IT (250 чел)          → бюджет 8%, success rate ~70%
├── Sales (200 чел)       → бюджет 7%, success rate ~65%
├── Finance (150 чел)     → бюджет 4%, success rate ~55%
├── Operations (250 чел)  → бюджет 5%, success rate ~70%
└── Admin (150 чел)       → бюджет 3%, success rate ~55%
```

**Конфликт требований:**

| Требование статистики | Реальность бизнеса | Результат |
|----------------------|-------------------|-----------|
| Нужно 200+ человек | Сегментация → 100-200 чел | ❌ Несовместимо |
| Единая большая группа | Разные бюджеты по функциям | ❌ Невозможно |
| 500+ для надежности | Типично 1000-5000 всего | ❌ Недостижимо |

**Вывод:** Даже компании с тысячой сотрудников не могут надежно использовать метод из-за необходимости функциональной сегментации.

### Кто МОЖЕТ использовать метод

**Применимо только для:**
- Гигантские гомогенные функции (1000+ сотрудников)
- Call-centers с 1,000+ операторов
- Массовое производство: 1,000+ рабочих

**Доля компаний:** < 20%

**НЕ применимо для:** 75-80% организаций (малый, средний бизнес и большинство компаний до 1,000 человек)


## Технические решения (Часть 2: Statistical Analysis & Validation)

## Используемые библиотеки

**Научные вычисления:**
* `pandas` — обработка табличных результатов Monte Carlo (350K+ групп)
* `numpy` — численные операции, векторизация расчетов

**Визуализация:**
* `matplotlib` — базовые графики, multi-panel dashboards
* `seaborn` — продвинутые визуализации (heatmaps, correlation matrices)

**Статистика и оптимизация:**
* `scipy.optimize.minimize` — оптимизация для piecewise regression
  * Метод: L-BFGS-B с bounded constraints
  * Цель: минимизация MSE при поиске breakpoints
* `scipy.stats` — корреляционный анализ (Pearson r), kernel density estimation
* `sklearn.metrics` — метрики качества регрессии (MSE для model selection)

## Ключевые технические решения

### 1. Univariate Analysis (Sections 1-6)

**Categorical encoding для корреляций:**
```python
# Ordinal encoding для сохранения порядка
size_bin_encoding = {'0-20': 1, '21-50': 2, ..., '500+': 6}
distribution_encoding = {'skewed_low': 1, 'uniform': 2, 'normal': 3, 'skewed_high': 4}
```

**Обоснование:** Pearson correlation требует числовые значения; ordinal encoding сохраняет смысловой порядок категорий.

**Adaptive binning стратегия:**
```python
# Для group size: неравномерные bins для захвата критических переходов
size_bins = [0, 20, 50, 100, 200, 500, np.inf]

# Для budget: равномерные bins для business relevance
budget_bins = [0, 5, 8, 10, 12, 15, np.inf]
```

### 2. Correlation Analysis (Section 7)

**Multi-dimensional correlation matrix:**
* Кодирование всех параметров как ordinal
* Расчет p-values для статистической значимости
* Интерпретация силы связи: |r| > 0.7 (Very Strong), > 0.4 (Strong), > 0.2 (Moderate)

**Ключевое открытие:** Group size объясняет 66.22 п.п. вариации (r=0.861), все остальные факторы — только 36 п.п.

### 3. Threshold Detection (Sections 8-9)

**Systematic ladder testing:**
```python
thresholds = [20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 400, 500, 750, 1000]

for threshold in thresholds:
    below = df[df['n_employees'] < threshold]
    above = df[df['n_employees'] >= threshold]
    success_boost = above['success_rate'].mean() - below['success_rate'].mean()
```

**Метрики:**
* Success Boost — разница в mean success rate выше/ниже порога
* Identifies "jumps" — пороги с максимальным приростом точности

### 4. Sophisticated Methods (Section 10)

**Piecewise Linear Regression:**

Оптимизация breakpoints с constraints:
```python
def fit_piecewise_linear(x, y, n_segments, min_segment_size=1000):
    # Constraint: минимум 1000 групп в каждом сегменте
    # Objective: minimize MSE
    # Method: L-BFGS-B optimization
```

**Numerical stability:**
* Нормализация x перед fitting: `x_norm = (x - mean) / std`
* Пересчет slopes/intercepts в исходный масштаб
* Обработка edge cases (сегменты с 1 наблюдением)

**Model Selection via AICc:**
```python
# Corrected Akaike Information Criterion для finite samples
AIC = n * log(MSE) + 2*k
AICc = AIC + (2*k*(k+1)) / (n - k - 1)
```

**Балансирует:**
* Goodness of fit (минимизация MSE)
* Model complexity (penalty за количество параметров)
* **Результат:** Оптимальное число сегментов = arg min(AICc)

**Monotonicity check:**
```python
is_monotone = np.all(np.diff(y_pred) >= -1e-6)  # Success rate не должен падать
```

### 5. Local Analysis at Breakpoints

**Gaussian-weighted local mean:**
```python
# Adaptive window size: 1% данных или 30-200 групп
window_size = max(30, min(200, int(len(x) * 0.01)))

# Gaussian weights для smooth локального усреднения
weights = np.exp(-((x_local - x0)**2) / (2 * sigma**2))
local_mean = np.sum(weights * y_local)
```

**Bootstrap confidence intervals:**
* 1000 resamples для оценки 95% CI
* Weighted bootstrap с Gaussian weights
* Результат: понимание uncertainty в критических точках

### 6. Robust Threshold Detection — 4 метода

**Cross-validation подход:**
1. **Raw First-Hit** — первая группа ≥80%
2. **Smoothed First-Hit** — rolling mean (window=100) ≥80%
3. **Quantile-Based** — где P50/P60/P70 ≥80%
4. **Piecewise Model Crossing** — где fitted model пересекает 80%

**Consensus decision:** Если все методы дают похожие результаты → высокая уверенность в пороге.

### 7. Advanced Visualizations

**4-panel comprehensive view:**
```python
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

# Panel 1: Detail view (0-2500) — фокус на критическом диапазоне
# Panel 2: Log scale — весь range данных
# Panel 3: Heatmap — 2D распределение (Size × Success Rate)
# Panel 4: Bar chart — success by segment с color coding
```

**Design principles:**
* Limited y-axis (0-105%) для удаления outliers
* Color coding: Red (<60%), Orange (60-80%), Green (≥80%)
* Breakpoint annotations с вертикальными линиями
* Median values внутри bars для robustness check

**Heatmap innovation:**
* Нормализация по столбцам → % within each segment
* Adaptive segment ranges на основе breakpoints
* Text annotations только для values >2% (избежание clutter)
* Adaptive text color: white на темном фоне, black на светлом

## Воспроизводимость

**Input:** `ALL_results.parquet` (результаты Monte Carlo из Части 1)

**Output:**
* Консольный вывод: summary statistics, correlation tables, threshold analysis
* Визуализации: `threshold_analysis.png` (300 DPI, high-resolution)
* Структурированные DataFrame для дальнейшего использования

**Время выполнения:**
* Loading data: 1-2 секунды (Parquet эффективен)
* Univariate analysis: ~5 секунд
* Piecewise regression (2-7 segments testing): ~30-60 секунд
  * Optimization computationally intensive для >350K observations
* Полный анализ: ~2-3 минуты на стандартном laptop

**Критические parameters:**
* `min_segment_size=1000` — статистическая значимость каждого сегмента
* `n_bootstrap=1000` — баланс точности CI и времени выполнения
* `window_pct=0.01` — локальное сглаживание (1% данных)

---

**Технический результат:** Математически строгая валидация метода с идентификацией точных пороговых значений (141-200 employees для перехода в зону надёжности) и количественной оценкой неопределённости для business decision making.

---

## Часть 3: Генетический алгоритм - решение проблемы

### Принципиальное отличие подходов

**Метод агрегированных распределений:**
```
Предполагает → распределение по CR и рейтингам
Рассчитывает → Бюджет как математическое ожидание
Проблема → Не учитывает реальное распределение
Результат → Работает только для 200+ человек
```

**Генетический алгоритм:**
```
Использует → Реальное распределение сотрудников по CR
Оптимизирует → Матрицу под конкретную популяцию
Валидирует → Через Monte Carlo с реальными данными
Результат → Работает для малых групп
```

### Архитектура решения

**1. Генетический алгоритм**
- **Популяция:** 1000 кандидатов merit matrices
- **Эволюция:** 500 поколений с ранней остановкой
- **Операторы:** Турнирная селекция, кроссовер, мутация
- **Улучшение:** Локальная оптимизация L-BFGS-B для топ-кандидатов

**2. Многокритериальная функция fitness**

```
Fitness = 0.70 × Budget_Score + 
          0.30 × Constraint_Score + 
          Bonus × Differentiation_Score
```

где:
- **Budget Score** — попадание в ±5% от целевого бюджета (70% веса)
- **Constraint Score** — соблюдение принципов справедливости (30% веса)
- **Differentiation** — видимость различий между рейтингами (бонус)

**3. Принципы справедливости (constraints)**

Встроенные жесткие ограничения:
- ✓ Высокий рейтинг → больше повышение (монотонность)
- ✓ Высокий CR → меньше повышение (монотонность)
- ✓ Минимальные шаги дифференциации (видимая разница)
- ✓ Защита от экстремальных значений
- ✓ Anchor cells (фиксация ключевых значений)

**4. Monte Carlo валидация**

Каждая матрица тестируется на **10,000+ симуляций**:
- Perturbation целевого распределения (Dirichlet)
- Стресс-сценарии различных распределений рейтингов
- Расчет success rate: % попаданий в бюджет ±5%

### Ключевые преимущества

**1. Адаптация к реальному распределению**

Алгоритм **знает** о фактическом распределении сотрудников:
- Если 70% имеют CR = 0.95-1.05 → это учитывается
- Матрица оптимизируется под конкретную популяцию
- **Результат:** Высокая точность для малых групп (50-100 чел)

**2. Гарантия справедливости**

Все принципы fair compensation автоматически соблюдаются через встроенные constraints.

**3. Автоматическая policy guidance**

Для каждого рейтинга генерируются рекомендации:

```
Rating 1: Target 10% | Допустимо 8.2-11.8%
Rating 2: Target 15% | Допустимо 13.1-16.9%
Rating 3: Target 50% | Допустимо 48.3-51.7%
Rating 4: Target 15% | Допустимо 13.2-16.8%
Rating 5: Target 10% | Допустимо 8.3-11.7%
```

Это реалистичные границы, при которых бюджет соблюдается (на основе Monte Carlo).

### Результаты применения

**Валидация на данных исследования:**

```
Данные: 395,325 сотрудников (Москва)
Группы: Company × Function
```

**Сравнение методов:**

| Размер группы | Aggregated Distributions | Genetic Algorithm | Улучшение |
|---------------|-------------------------|-------------------|-----------|
| 50-100 | ~55% | ~85% | **+30%** |
| 100-200 | ~70% | ~92% | **+22%** |
| 200+ | ~90% | ~96% | +6% |

**Ключевой результат:** Улучшение на +22-30 процентных пунктов для групп 50-200 человек.

### Практическое применение

**Workflow:**

1. **Подготовка:** Загрузить данные сотрудников (base_salary, CR)
2. **Настройка:** Задать целевой бюджет, распределение рейтингов, constraints
3. **Оптимизация:** Запуск алгоритма (~2-5 минут)
4. **Результат:** Top-20 оптимальных матриц с fitness scores и policy guidance

**Пример выходного результата:**

```
ОПТИМАЛЬНАЯ МАТРИЦА (Success Rate: 94.2%)

                Rating 1  Rating 2  Rating 3  Rating 4  Rating 5
CR [0.0-0.80)      3.5%      6.8%     10.2%     13.5%     16.8%
CR [0.80-0.90)     2.8%      5.9%      9.1%     12.2%     15.3%
CR [0.90-1.10)     2.1%      5.0%      8.0%     11.0%     14.0%
CR [1.10-1.20)     1.4%      4.1%      6.9%      9.6%     12.3%
CR [1.20-∞)        0.7%      3.2%      5.8%      8.3%     10.8%

Metrics:
├─ Fitness: 0.9547
├─ Budget Score: 0.9823 (98.2% scenarios within ±5%)
├─ Constraint Score: 0.9871 (все принципы соблюдены)
└─ Mean Deviation: 1.23% от целевого бюджета
```

---

## Общие выводы

### Трехэтапный проект

**Этап 1: Валидация (Monte Carlo)**
- Метод агрегированных распределений математически корректен
- Требует 200+ сотрудников для 80%+ точности
- Непригоден для 75-80% компаний из-за необходимости сегментации

**Этап 2: Анализ**
- Размер группы объясняет ~85-90% вариации в точности
- Конфигурация матрицы влияет минимально (±6%)
- Выявлен фундаментальный парадокс: статистические требования vs бизнес-реальность

**Этап 3: Решение (Генетический алгоритм)**
- Работает для групп от 50 человек
- Улучшение на +22-30% для малых/средних групп
- Применим для 75-80% компаний

### Практическая ценность

**Для HR:**
- Понимание ограничений традиционного метода
- Data-driven подход доступен для типичных организационных структур
- Автоматизация создания справедливых merit matrices

**Для Finance:**
- Количественная оценка бюджетных рисков
- Предсказуемость: известна вероятность попадания в бюджет

**Для бизнеса:**
- Решение применимо для компаний любого размера (100-100,000 человек)
- Гарантия справедливости через автоматические constraints
- Эффективное распределение бюджета и его контроль

### Научный вклад

1. **Первое систематическое исследование** точности aggregated distributions method на крупномасштабных реальных данных (383K+ сотрудников)

2. **Выявление фундаментального ограничения** широко используемого метода и количественное определение границ применимости

3. **Разработка практического решения** (генетический алгоритм), преодолевающего выявленные ограничения

4. **Мост между теорией и практикой:** коммерческое применение академических методов оптимизации в управлении вознаграждением

---

## Статус проекта

**Академическое исследование** (Этапы 1-2): Завершено
- Валидация метода на данных 395,325 сотрудников
- Анализ результатов и выявление ограничений
- Публикация результатов (данный документ)

**Коммерческое решение** (Этап 3):
- Генетический алгоритм разработан и валидирован
- Доступен исключительно клиентам Lens Consulting
- Применяется в консалтинговых проектах по C&B
