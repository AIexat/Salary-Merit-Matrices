# Бюджетирование повышений зарплат: Валидация метода, Анализ ограничений и Оптимизаци (Merit Matrix Budgeting: Validation, Analysis, and Optimization)

## Автор
Алексей Иванов  
Итоговый проект программы «Специалист по Data Science», НИУ ВШЭ

---

## Содержание
1. [Краткое резюме](#краткое-резюме)
2. [Описание проблемы](#описание-проблемы)
3. [Структура проекта](#структура-проекта)
4. [Часть 1: Monte Carlo Validation](#часть-1-monte-carlo-validation)
5. [Часть 2: Анализ результатов](#часть-2-анализ-результатов)
6. [Часть 3: Генетический алгоритм - решение проблемы](#часть-3-генетический-алгоритм---решение-проблемы)
7. [Общие выводы](#общие-выводы)

---

## Краткое резюме

### TL;DR

**Исследуемый вопрос:** Насколько надежен широко используемый метод агрегированных распределений для расчета бюджета повышений зарплат?

**Метод:** Monte Carlo симуляция на реальных рыночных данных (383,000+ сотрудников, 300+ компаний в Москве, референтная дата: 1 мая 2025). 2,921 функциональная группа, 350,520 тестовых сценариев.

**Главные выводы:** 
1. Метод математически корректен, но требует **300+ сотрудников** для минимальной надежности (92% success rate), **400+** для высокой надежности (98%)
2. Выявлена **"зона опасной иллюзии" (141-299 чел)**: обманчиво высокие средние SR (81-88%) маскируют реальный риск — 22-46% групп всё равно выходят за бюджет ±5%
3. **Парадокс высоких средних:** средняя SR=88% не гарантирует надежность из-за бимодального распределения
4. Метод **непригоден для 70-75% бизнес-контекстов** из-за недостижимости порога 300+ при функциональной сегментации

**Решение:** Разработан генетический алгоритм, который преодолевает выявленные ограничения — работает для групп от 50 человек с улучшением надежности на +22-30%, применим для 70-80% компаний.

---

## Описание проблемы

### Что такое Merit Matrix?

**Merit Matrix (матрица повышений)** — инструмент HR для определения процента повышения зарплаты на основе двух параметров:

1. **Compa Ratio (CR)** — отношение текущей зарплаты к рыночной медиане или серединной точки в структуре вознаграждения (midpoint) для сотрудника:
   - CR < 1.0: ниже рынка
   - CR = 1.0: на рынке
   - CR > 1.0: выше рынка

2. **Performance Rating** — оценка эффективности сотрудника, определяемая непосредственным руковожителем (обычно от 3 до 7)

**Пример Merit Matrix 5×5:**

| CR / Rating | R1 | R2 | R3 | R4 | R5 |
|-------------|----|----|----|----|----| 
| < 0.80 | 6% | 10% | 14% | 17% | 20% |
| 0.80-0.90 | 5% | 9% | 12% | 15% | 18% |
| 0.90-1.10 | 3% | 7% | 10% | 13% | 16% |
| 1.10-1.20 | 2% | 5% | 8% | 11% | 13% |
| > 1.20 | 1% | 3% | 6% | 8% | 10% |

### Фундаментальная проблема прогнозирования бюджета

**Проблема временного разрыва:**
- **Q3-Q4:** HR утверждает бюджет на повышения
- **Q1:** Менеджеры присваивают рейтинги сотрудникам
- **Разрыв:** 3-6 месяцев между планированием и реализацией

**Что HR знает:**
- Выделенный бюджет на повышение заработных плат для группы сотрудников
- Merit Matrix (проценты повышений), которая разрабатывается самим HR
- Распределение сотрудников по диапазоном CR
- Целевое распределение рейтингов (10%-20%-40%-20%-10%), которое задается политикой или историческими данными

**Что HR не знает и не контролирует:**
- Какому конкретно сотруднику (с каким CR) какой рейтинг будет присвоен
- Будут ли менеджеры строго придерживаться заданного распределения

### Метод агрегированных распределений

HR использует формулу:

```
Ожидаемый бюджет = Σ P(CR_bin_i) × P(Rating_j) × Merit(i,j)
```

где:
- `P(CR_bin_i)` — доля сотрудников в CR-бине (известно из данных)
- `P(Rating_j)` — целевая доля рейтинга j (задается политикой)
- `Merit(i,j)` — процент из матрицы

**Почему "агрегированных распределений"?**
- Метод **агрегирует** (объединяет) два независимых распределения:
  1. Распределение сотрудников по CR bins (известно из данных)
  2. Целевое распределение рейтингов (задается HR политикой)

- Бюджет рассчитывается как математическое ожидание по этим распределениям

**Вопрос исследования:** Насколько точен этот метод в реальности? Как часто реальный бюджет попадает в заданный диапазон (например, ±5% от расчета по агрегированным распределениям)?
---

## Структура проекта
```
merit_matrix_project/
├── data/
│   └── Moscow.parquet.gzip                    # Подготовленные данные (конфиденциальны)
├── notebooks/
│   ├── 01_Monte_Carlo_Simulation.ipynb        # Часть 1: Симуляции
│   ├── 02_Results_Analysis.ipynb              # Часть 2: Анализ
│   └── 03_Genetic_Algorithm_Optimizer.ipynb   # Часть 3: Оптимизация
├── visualizations/
│   └── threshold_analysis.png                 # Графики
└── README.md                                  # Данный документ
```


## Часть 1: Monte Carlo Validation

### Цель

Проверить точность метода агрегированных распределений через симуляцию реальных сценариев присвоения рейтингов.

### Методология

**Для каждой группы сотрудников (Company × Function):**

1. **Рассчитать baseline:** бюджет по методу агрегированных распределений
2. **Провести 50,000 симуляций:**
   - Случайно присвоить рейтинги согласно целевому распределению
   - Добавить реалистичные вариации (Dirichlet distribution)
   - Рассчитать реальный бюджет по merit matrix
   - Проверить попадание в диапазон ±5%
3. **Рассчитать success rate:** % симуляций в пределах допуска

### Тестирование робастности

Для проверки устойчивости метода анализ проведен на **120 различных конфигурациях**:
- 6 вариантов CR bins (2-7)
- 5 вариантов ratings (3-7)
- 4 типа распределений (normal, skewed_high, skewed_low, uniform)

**Цель:** Убедиться, что результаты не зависят от конкретного дизайна матрицы.

### Данные

**Источник:** Исследование по вознаграждению Lens Consulting c референтной датой 1 мая 2025
- **Регион:** Москва
- **Сотрудников:** 383 483 сотрудника
- **Группировка:** Company × Function (2921 групп)
- **Размеры групп:** от 10 до 5000+ сотрудников

### Технические решения

#### Используемые библиотеки

**Обработка данных:**
- `pandas` — группировка по Company×Function, расчет медиан
- `numpy` — векторизованные операции с массивами

**Статистическое моделирование:**
- `scipy.stats.dirichlet` — моделирование вариаций распределения рейтингов
  - Параметр `concentration=250` создает отклонения ±2-3% от целевого распределения
  - Имитирует реальность: менеджеры следуют политике, но с небольшими исключениями

**Оптимизация производительности:**
- `numba` — JIT-компиляция критического цикла Monte Carlo
  - Ускорение в ~100 раз vs чистый Python
  - Параллелизация на все CPU cores (`@jit(parallel=True)`)
  - **Результат:** ~2-3 секунды на группу из 100 сотрудников (50K итераций)

#### Ключевые технические решения

**1. Подготовка данных**

```python
# Расчет Midpoint (медиана зарплат по Company×Function×Grade)
midpoints = df.groupby(['Company', 'Function', 'Grade'])['BP'].median()

# Расчет Compa Ratio для каждого сотрудника
CR = Base_Pay / Midpoint

# Агрегация: убираем Grade (CR уже учитывает позицию относительно грейда)
```

**Обоснование:** Использование медианы для устойчивости к выбросам; стандартная практика в compensation management.

**2. Генерация Merit Matrices**

Автоматическая генерация матриц с соблюдением принципов справедливости:
- Высокий рейтинг → больше повышение (70% влияния)
- Высокий CR → меньше повышение (30% влияния)
- Диапазон: 1% (worst case) до 20% (best case)

**3. Оптимизированный Monte Carlo**

**Векторизация:** Все случайные числа генерируются до основного цикла
```python
# Pre-generation для 50,000 итераций
all_sampled_probs = dirichlet.rvs(alpha, size=50000)  # Dirichlet samples
all_random_values = np.random.random((50000, n_employees))  # Uniform RV
```

**Numba JIT-компиляция:**
```python
@jit(nopython=True, parallel=True)
def _run_simulations_weighted(...):
    for sim_idx in prange(n_simulations):  # Параллельный цикл
        # Присвоение рейтингов
        # Расчет merit amounts
        # Проверка попадания в tolerance
```

**4. Класс FlexibleMeritAnalyzer**

Универсальный анализатор для матриц любого размера (2×3 до 7×7):
- `calculate_heuristic_budget()` — реализация метода агрегированных распределений
- `run_monte_carlo_for_group()` — 50,000 симуляций для одной группы
- `process_dataframe()` — обработка всех групп Company×Function с автофильтрацией малых выборок

#### Воспроизводимость

**Сохранение результатов:**
- Формат: Parquet (компактный, быстрый)
- Naming: `merit_analysis_{config_name}_{timestamp}.parquet`
- Метаданные: config_name, n_cr_bins, n_ratings, distribution_type

**Время выполнения:**
- Одна конфигурация (~300 групп): 10-15 минут
- Все 120 конфигураций: 2-4 часа на стандартном laptop (8 cores, 16GB RAM)

---

## Часть 2: Анализ результатов

### Корреляционная матрица
![Correlation Analysis](correlation.png)

### Анализ Success rate в зависимости от размера группы
![Threshold Analysis](threshold_analysis.png)

### Детальный сегментированный анализ по достижению допустимого уровня success rate (>= 80%)
![Detailed Analysis](detailed_segment_analysis.png)



# Success Rate Analysis: Комплексный анализ порогов надежности

**Примечание:** Success Rate (SR) — это процент итераций Monte Carlo симуляции, в которых итоговый бюджет попал в допустимый коридор ±5% от целевого значения. Например, SR=80% означает, что в 8 случаях из 10 (или 8 годах из 10) компания попадет в установленный бюджетный коридор. При целевом бюджете 100,000,000 допустимый диапазон составляет 95,000,000 - 105,000,000. Success Rate по сути показывает **надежность метода**: чем выше SR, тем реже компания будет выходить за пределы запланированного бюджета.

## Success rate по сегментам

| Сегмент | Размер группы | Mean/Median SR | Всего групп | **% групп SR≥80%** | **% групп SR≥90%** | Интерпретация |
|---------|---------------|----------------|-------------|-------------------|-------------------|---------------|
| **BP1** | <12 | 28.4% / 27.1% | 30,000 | **0.0%** | **0.0%** | ❌ **Катастрофа**: 100% групп выходят за бюджет |
| **BP2** | 12-16 | 32.2% / 30.7% | 56,880 | **0.0%** | **0.0%** | ❌ **Критично**: 100% групп за пределами бюджета |
| **BP3** | 17-24 | 38.1% / 36.8% | 57,840 | **0.0%** | **0.0%** | ❌ **Провал**: 100% групп выходят за бюджет |
| **BP4** | 25-38 | 45.5% / 44.4% | 54,600 | **0.0%** | **0.0%** | ❌ **Неприемлемо**: 100% групп за бюджетом |
| **BP5** | 39-63 | 55.2% / 54.2% | 49,680 | **0.4%** | **0.0%** | ❌ **Крайне рискованно**: 99.6% групп провалились |
| **BP6** | 64-140 | 68.0% / 68.8% | 51,240 | **26.4%** | **4.0%** | ❌ **Неприемлемо**: 3 из 4 групп за бюджетом |
| **BP7** | 141-199 | 81.2% / 82.7% | 14,880 | **54.2%** | **35.7%** | ⚠️ **"Зона иллюзии"**: Почти половина провалилась |
| **BP8** | 200-299 | 88.2% / 90.5% | 11,520 | **78.2%** | **50.9%** | ⚠️ **Недостаточно**: Каждая 5-я группа провалилась |
| **BP9** | 300-399 | 92.5% / 94.5% | 6,480 | **92.3%** | **67.4%** | ✅ **МИНИМАЛЬНО НАДЕЖНО**: ~8% групп провалились |
| **BP10** | 400-499 | 95.6% / 97.6% | 2,160 | **98.4%** | **86.6%** | ✅ **НАДЕЖНО**: Только 1.6% провалились |
| **BP11** | 500-999 | 97.7% / 99.4% | 8,040 | **99.3%** | **95.0%** | ✅ **Очень надежно**: <1% провалились |
| **BP12** | ≥1000 | 99.8% / 100.0% | 7,200 | **100.0%** | **99.9%** | ✅ **Абсолютно надежно**: 0% провалились |

---

## Ключевые выводы

### КРИТИЧЕСКИЕ ПОРОГИ НАДЕЖНОСТИ

#### Для достижения SR≥80% у 80%+ групп:
- **300-399 сотрудников** — первый порог, где **92.3% групп** достигают SR≥80%
- Это означает, что только **7.7% групп** всё ещё выходят за бюджет ±5%

#### Для достижения SR≥80% у 95%+ групп:
- **400-499 сотрудников** — **98.4% групп** достигают SR≥80%
- Только **1.6% групп** провалятся (менее 2 из 100)

#### Для достижения SR≥90% у 80%+ групп:
- **400-499 сотрудников** — **86.6% групп** достигают SR≥90%
- Это премиум-уровень надежности для строгого бюджетного контроля

#### Для абсолютной надежности:
- **1000+ сотрудников** — **100% групп** достигают SR≥80%, **99.9%** достигают SR≥90%

### КРИТИЧЕСКОЕ ОТКРЫТИЕ: "Зона иллюзии надежности"

**Сегменты 141-299 сотрудников — самая опасная зона:**

| Размер группы | Mean SR | % групп SR≥80% | % групп SR≥90% | Реальность |
|---------------|---------|----------------|----------------|------------|
| 141-199 | 81.2% | **54.2%** | **35.7%** | ⚠️ **Почти половина групп (45.8%) всё равно провалилась** |
| 200-299 | 88.2% | **78.2%** | **50.9%** | ⚠️ **Каждая 5-я группа (21.8%) за бюджетом** |

**Почему это критически опасно:**
1. **Обманчиво высокие средние** (81-88%) создают **ложное ощущение надежности**
2. На практике **21.8-45.8% групп регулярно выходят за бюджет** ±5%
3. Компании видят "среднюю" SR 81-88% и ошибочно считают это достаточным
4. **Даже при 200-299 сотрудниках** только 50.9% групп достигают SR≥90%

### ⚠️ Парадокс высоких средних значений

**Почему средняя SR=88.2%, но только 78.2% групп достигают SR≥80%?**

**Математическое объяснение:**
- **Бимодальное распределение**: Небольшая часть групп показывает SR=95-100%, сильно завышая среднее
- **Выбросы наверху**: 20-30% "звездных" групп с SR=90-100% тянут среднее вверх
- **Но**: Остальные 70-80% групп распределены в диапазоне 60-85%, многие ниже 80%

**Практический пример** (сегмент 200-299 чел):
```
Средняя SR = 88.2% (выглядит отлично!)
НО реальное распределение:
├── 50.9% групп: SR≥90% (успешные) ✅
├── 27.3% групп: SR 80-89% (на грани) ⚠️
└── 21.8% групп: SR<80% (провал) ❌

Компания видит "88%" и думает, что всё хорошо.
Реальность: каждая 5-я группа регулярно выходит за бюджет.
```

### Реальная картина по порогам

| Порог | % групп SR≥80% | % групп SR≥90% | Практический смысл |
|-------|----------------|----------------|-------------------|
| <64 чел | 0-0.4% | 0% | ❌ **Безнадежно**: 99.6-100% групп провалились |
| 64-140 чел | 26.4% | 4.0% | ❌ **Критично**: 3 из 4 групп провалились |
| 141-199 чел | 54.2% | 35.7% | ⚠️ **"Зона иллюзии"**: почти половина провалилась |
| 200-299 чел | 78.2% | 50.9% | ⚠️ **Недостаточно**: каждая 5-я группа провалилась |
| **300-399 чел** | **92.3%** | **67.4%** | ✅ **МИНИМАЛЬНО НАДЕЖНО**: только 7.7% провалились |
| **400-499 чел** | **98.4%** | **86.6%** | ✅ **НАДЕЖНО**: только 1.6% провалились |
| 500-999 чел | 99.3% | 95.0% | ✅ **Очень надежно**: <1% провалились |
| 1000+ чел | 100.0% | 99.9% | ✅ **Абсолютная надежность** |

### Итоговые критические пороги

**Уровни надежности:**

1. **Минимально приемлемый** (90% групп успешны): **300-399 сотрудников**
   - 92.3% групп с SR≥80%
   - Только 7.7% провалов

2. **Бизнес-надежный** (95% групп успешны): **400-499 сотрудников**
   - 98.4% групп с SR≥80%
   - Только 1.6% провалов

3. **Премиум** (99% групп успешны): **500-999 сотрудников**
   - 99.3% групп с SR≥80%
   - Менее 1% провалов

4. **Абсолютный** (100% групп успешны): **1000+ сотрудников**
   - 100% групп с SR≥80%
   - 0% провалов

---

## Парадокс практической применимости

**Реальность бизнеса:** Компании **не могут** применять единую матрицу ко всей организации из-за необходимости функциональной сегментации.

### Пример: компания 600 сотрудников
```
Сегментация по функциям:
├── Operations (180 чел)  → бюджет 6%, ~54% шанс SR≥80% ⚠️
├── Sales (150 чел)       → бюджет 7%, ~54% шанс SR≥80% ⚠️
├── IT (120 чел)          → бюджет 8%, ~26% шанс SR≥80% ❌
├── Finance (60 чел)      → бюджет 4%, ~26% шанс SR≥80% ❌
├── Admin/HR (50 чел)     → бюджет 3%, ~0.4% шанс SR≥80% ❌
└── Marketing (40 чел)    → бюджет 5%, ~0.4% шанс SR≥80% ❌
```

**Критическая проблема:** 
- **Ни одна функция не достигает порога 300+ человек** (минимально надежный)
- **Ни одна функция не имеет 90%+ вероятность успеха**
- Даже крупнейшие функции (150-180 чел) имеют только **54.2% шанс** достичь SR≥80%

**Реальный результат по функциям:**

| Функция | Размер | % вероятность SR≥80% | Ожидаемый результат |
|---------|--------|---------------------|---------------------|
| Operations | 180 | 54.2% | ⚠️ В половине лет выйдут за бюджет |
| Sales | 150 | 54.2% | ⚠️ В половине лет выйдут за бюджет |
| IT | 120 | 26.4% | ❌ В 3 из 4 лет выйдут за бюджет |
| Finance | 60 | 26.4% | ❌ В 3 из 4 лет выйдут за бюджет |
| Admin/HR | 50 | 0.4% | ❌ Практически всегда за бюджетом |
| Marketing | 40 | 0.4% | ❌ Практически всегда за бюджетом |

**Средневзвешенный успех по компании:**
- Только **~30-35% функций** попадут в бюджет в типичный год
- **65-70% функций** выйдут за бюджет ±5%

### Конфликт требований

| Требование статистики | Реальность бизнеса | Результат |
|----------------------|-------------------|-----------|
| **Нужно 300+ для 92% надежности** | **Ни одной функции >300 чел** | ❌ **100% функций ненадежны** |
| **Нужно 400+ для 98% надежности** | **Ни одной функции >400 чел** | ❌ **100% функций высокий риск** |
| Нужно 200+ для 78% шанса | Только 1 функция >200 чел | ❌ 83% функций провалятся |
| Нужно 141+ для 54% шанса | Только 2 функции >141 чел | ❌ 67% функций <50% шанс |
| Единая группа 600 чел | 6 разных бюджетов | ❌ Невозможно объединить |

---

## Масштабирование проблемы

### Компании 300-1,000 человек (средний бизнес)
```
Типичная структура:
├── 1-3 крупные функции (150-250 чел) → ~54-78% шанс SR≥80% ⚠️
├── 4-8 средних функций (60-140 чел)  → ~0.4-26% шанс SR≥80% ❌
└── 6-12 малых функций (<60 чел)      → ~0% шанс SR≥80% ❌
```

**КРИТИЧНО:** Ни одна функция не достигает надежного порога 300+

**Практический итог:** 
- **90-95% функций НЕ достигнут SR≥80% надежно**
- Даже "крупные" функции имеют только 50-80% шанс успеха в конкретный год
- В среднем только **1-2 функции из 10-15** будут стабильно успешны
- **Остальные 8-13 функций регулярно выходят за бюджет**

**Реальный сценарий** (компания 800 чел):
```
Функциональная структура (12 функций):
├── Operations (220 чел)      → 78% шанс ⚠️  → выйдет за бюджет в 1 из 5 лет
├── Sales (180 чел)           → 54% шанс ⚠️  → выйдет за бюджет в 1 из 2 лет
├── IT (140 чел)              → 26% шанс ❌  → выйдет за бюджет в 3 из 4 лет
├── Customer Service (100)    → 26% шанс ❌  → выйдет за бюджет в 3 из 4 лет
├── 8 других функций (<80)    → 0-4% шанс ❌ → почти всегда за бюджетом

Ожидаемый результат в типичный год:
✅ 1-2 функции попадут в бюджет (8-17%)
⚠️ 2-3 функции на грани (17-25%)
❌ 7-9 функций выйдут за бюджет (58-75%)
```

### Компании 1,000-3,000 человек
```
Типичная структура:
├── 2-4 крупные функции (250-450 чел)  → ~78-98% шанс SR≥80% ⚠️/✅
├── 6-10 средних функций (100-200 чел) → ~26-78% шанс SR≥80% ❌/⚠️
└── 12-20 малых функций (<100 чел)     → ~0-26% шанс SR≥80% ❌
```

**ВАЖНО:** Только 1-2 функции могут достичь 400-500 чел (надежная зона 98%)

**Практический итог:**
- **70-80% функций всё ещё работают с высоким риском** (<90% надежность)
- Только крупнейшие Operations (300-450 чел) приближаются к надежности
- Большинство функций в диапазоне 100-250 чел имеют только 26-78% шанс успеха

**Реальный сценарий** (компания 2,000 чел):
```
Функциональная структура (18 функций):
├── Operations (400 чел)      → 98% шанс ✅  → надежно
├── Sales (300 чел)           → 92% шанс ✅  → почти надежно
├── IT (250 чел)              → 78% шанс ⚠️  → каждый 5-й год провал
├── 5 функций (100-200 чел)   → 26-78% ⚠️/❌ → частые провалы
├── 10 функций (<100 чел)     → 0-26% ❌     → регулярные провалы

Ожидаемый результат в типичный год:
✅ 2-3 функции надежно в бюджете (11-17%)
⚠️ 4-6 функций на грани (22-33%)
❌ 9-12 функций выйдут за бюджет (50-67%)
```

### Компании 3,000-10,000 человек (крупный бизнес)
```
Типичная структура:
├── 3-6 гигантских функций (400-1000+ чел) → ~98-100% шанс SR≥80% ✅
├── 8-15 крупных функций (200-400 чел)     → ~78-98% шанс SR≥80% ⚠️/✅
├── 15-25 средних функций (80-200 чел)     → ~26-78% шанс SR≥80% ❌/⚠️
└── 20-40 малых функций (<80 чел)          → ~0-26% шанс SR≥80% ❌
```

**Практический итог:**
- **60-70% функций всё ещё работают с высоким риском** (<90% надежность)
- Только ~20-30% функций достигают надежного порога 300-400+
- Даже в крупных корпорациях большинство поддерживающих функций слишком малы

**Реальный сценарий** (компания 5,000 чел):
```
Функциональная структура (35 функций):
├── 5 гигантских (500-800 чел)  → 99-100% ✅  → абсолютно надежно
├── 8 крупных (250-400 чел)     → 78-98% ⚠️/✅ → в основном надежно
├── 15 средних (80-200 чел)     → 26-78% ❌/⚠️ → частые провалы
├── 7 малых (<80 чел)           → 0-26% ❌     → регулярные провалы

Ожидаемый результат в типичный год:
✅ 10-13 функций надежно (29-37%)
⚠️ 8-12 функций на грани (23-34%)
❌ 10-17 функций провалятся (29-49%)
```

---

## Итоговые выводы по функциям

Метод **фундаментально ненадежен** для подавляющего большинства функциональных структур:

| Тип функции | Типичный размер | % групп SR≥80% | Практическая надежность |
|-------------|----------------|----------------|------------------------|
| **Admin, HR, Legal, Marketing** | 20-80 чел | 0-26% | ❌ **74-100% лет провалятся** |
| **Finance, Procurement, Quality** | 60-120 чел | 0.4-26% | ❌ **74-99.6% лет провалятся** |
| **IT, Customer Service** | 100-180 чел | 26-54% | ❌ **46-74% лет провалятся** |
| **Sales (средние компании)** | 150-250 чел | 54-78% | ⚠️ **22-46% лет провалятся** |
| **Operations (средние компании)** | 200-300 чел | 78-92% | ⚠️ **8-22% лет провалятся** |
| **Operations (крупные компании)** | 300-500 чел | 92-98% | ✅ **2-8% лет провалятся** |
| **Гигантские функции** | 500+ чел | 99-100% | ✅ **0-1% лет провалятся** |

---

## Кто МОЖЕТ использовать метод надежно

### Надежно (92%+ групп успешны)

**Функции 300+ сотрудников:**
- Крупные Operations/Manufacturing (300-800 чел)
- Гигантские Sales в B2B корпорациях (300-600 чел)
- Массивные IT-отделы (300-700 чел)
- Огромные Call-centers (400-1000+ операторов)
- Логистические центры (300-600 чел)

**Доля компаний/функций:** **<15% всех бизнес-контекстов**

### "Зона опасной иллюзии" (54-78% успеха)

**Функции 141-299 человек:**
- Средние Operations (150-250 чел)
- Крупные IT-отделы (150-250 чел)
- Большие Sales-команды (150-250 чел)

**КРИТИЧЕСКИ ОПАСНО:** 
- Средние SR 81-88% создают **ложное ощущение надежности**
- **На практике 22-46% групп регулярно провалятся**
- Компании видят "88% среднее" и не осознают, что каждая 4-5-я группа выходит за бюджет
- Это **самый обманчивый диапазон** — выглядит хорошо, работает плохо

**Доля:** ~15-20% бизнес-контекстов

### Критически ненадежно (0-54% успеха)

**Охватывает:**
- **75%+ всех функциональных подразделений** (<141 чел)
- **90%+ компаний малого/среднего бизнеса** (<1000 чел)
- **65-75% функций в крупных компаниях** (1000-10000 чел)

**Итого:** **~70-75% всех бизнес-контекстов**

---

## 💡 Ключевой инсайт: "Закон 300+"

### Правило надежности

**Минимальный порог:** **300+ сотрудников** для 92.3% надежности (только 7.7% провалов)

**Рекомендуемый порог:** **400+ сотрудников** для 98.4% надежности (только 1.6% провалов)

**Абсолютная надежность:** **1000+ сотрудников** для 100% надежности (0% провалов)

### Практическая реальность

#### Кто может достичь порога 300+?

**Анализ по размеру компаний:**
- **<1,000 чел:** Обычно 0-1 функция ≥300 чел (**0-10% функций**)
- **1,000-3,000 чел:** Обычно 1-3 функции ≥300 чел (**10-20% функций**)
- **3,000-10,000 чел:** Обычно 3-8 функций ≥300 чел (**15-30% функций**)
- **10,000+ чел:** Обычно 10-20 функций ≥300 чел (**30-40% функций**)

**Итоговая статистика:**
- ✅ Менее **15-20% всех бизнес-контекстов** достигают порога 300+
- ⚠️ **20-25% контекстов** в "зоне иллюзии" (141-299 чел)
- ❌ **55-65% контекстов** критически малы (<141 чел)

### Парадокс метода

**Метод математически корректен**, но **практически неприменим** для подавляющего большинства реальных бизнес-структур.

**Четыре фундаментальных проблемы:**

1. **Проблема размера**: 80-85% функций не достигают порога 300+ человек

2. **Проблема сегментации**: Компании обязаны разбивать персонал на функции с разными бюджетами, что делает невозможным создание больших гомогенных групп

3. **Проблема "зоны иллюзии"** (141-299 чел): 
   - Обманчиво высокие средние (81-88% SR)
   - На практике 22-46% групп всё равно провалятся
   - Самая опасная зона — создает ложное чувство безопасности

4. **Проблема масштаба**: Даже компании в 5,000-10,000 человек имеют только 20-30% функций с достаточной надежностью

---

## 📊 Финальная матрица применимости

| Размер компании | % функций ≥300 чел | % функций в "зоне иллюзии" | % функций критично малы | Применимость метода |
|-----------------|-------------------|---------------------------|------------------------|---------------------|
| <500 чел | 0% | 10-20% | 80-90% | ❌ **Неприменимо** (0% надежных функций) |
| 500-1,000 чел | 0-10% | 15-25% | 65-85% | ❌ **Критично** (почти все функции ненадежны) |
| 1,000-3,000 чел | 10-20% | 20-30% | 50-70% | ⚠️ **Ограниченно** (только 1-3 функции надежны) |
| 3,000-10,000 чел | 15-30% | 25-35% | 35-60% | ⚠️ **Частично** (3-8 функций надежны, остальные нет) |
| 10,000+ чел | 30-40% | 30-35% | 25-40% | ✅ **Применимо** (10-20 функций надежны, но 60-70% всё равно ненадежны) |

**Общий вывод:** Метод применим только для **10-20% крупнейших функций** в **15-20% крупнейших компаний**. Для остальных **70-75% бизнес-контекстов** метод **фундаментально ненадежен**.

---

### Влияние других факторов (минимальное)

**Конфигурация матрицы:**
- Количество CR bins (2-7): разброс success rate 47-53% (±6%)
- Количество Ratings (3-7): разброс 48-54% (±6%)
- **Вывод:** Размер матрицы практически не влияет на точность

**Тип распределения рейтингов:**
- Разброс: 43-57% (14 процентных пунктов)
- **Вывод:** Умеренное влияние, значительно меньше размера группы

**Сводная таблица влияния:**

| Фактор | Разброс Success Rate | Влияние |
|--------|---------------------|---------|
| **Размер группы** | **33% → 98%** (65%) | ⭐⭐⭐⭐⭐ Критический |
| Тип распределения | 43% → 57% (14%) | ⭐⭐ Умеренное |
| Количество CR bins | 47% → 53% (6%) | ⭐ Минимальное |
| Количество Ratings | 48% → 54% (6%) | ⭐ Минимальное |

**Вывод:** Размер группы объясняет ~85-90% вариации в точности.


## Технические решения (Часть 2: Statistical Analysis & Validation)

## Используемые библиотеки

**Научные вычисления:**
* `pandas` — обработка табличных результатов Monte Carlo (350K+ групп)
* `numpy` — численные операции, векторизация расчетов

**Визуализация:**
* `matplotlib` — базовые графики, multi-panel dashboards
* `seaborn` — продвинутые визуализации (heatmaps, correlation matrices)

**Статистика и оптимизация:**
* `scipy.optimize.minimize` — оптимизация для piecewise regression
  * Метод: L-BFGS-B с bounded constraints
  * Цель: минимизация MSE при поиске breakpoints
* `scipy.stats` — корреляционный анализ (Pearson r), kernel density estimation
* `sklearn.metrics` — метрики качества регрессии (MSE для model selection)

## Ключевые технические решения

### 1. Univariate Analysis (Sections 1-6)

**Categorical encoding для корреляций:**
```python
# Ordinal encoding для сохранения порядка
size_bin_encoding = {'0-20': 1, '21-50': 2, ..., '500+': 6}
distribution_encoding = {'skewed_low': 1, 'uniform': 2, 'normal': 3, 'skewed_high': 4}
```

**Обоснование:** Pearson correlation требует числовые значения; ordinal encoding сохраняет смысловой порядок категорий.

**Adaptive binning стратегия:**
```python
# Для group size: неравномерные bins для захвата критических переходов
size_bins = [0, 20, 50, 100, 200, 500, np.inf]

# Для budget: равномерные bins для business relevance
budget_bins = [0, 5, 8, 10, 12, 15, np.inf]
```

### 2. Correlation Analysis (Section 7)

**Multi-dimensional correlation matrix:**
* Кодирование всех параметров как ordinal
* Расчет p-values для статистической значимости
* Интерпретация силы связи: |r| > 0.7 (Very Strong), > 0.4 (Strong), > 0.2 (Moderate)

**Ключевое открытие:** Group size объясняет 66.22 п.п. вариации (r=0.861), все остальные факторы — только 36 п.п.

### 3. Threshold Detection (Sections 8-9)

**Systematic ladder testing:**
```python
thresholds = [20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 250, 300, 400, 500, 750, 1000]

for threshold in thresholds:
    below = df[df['n_employees'] < threshold]
    above = df[df['n_employees'] >= threshold]
    success_boost = above['success_rate'].mean() - below['success_rate'].mean()
```

**Метрики:**
* Success Boost — разница в mean success rate выше/ниже порога
* Identifies "jumps" — пороги с максимальным приростом точности

### 4. Sophisticated Methods (Section 10)

**Piecewise Linear Regression:**

Оптимизация breakpoints с constraints:
```python
def fit_piecewise_linear(x, y, n_segments, min_segment_size=1000):
    # Constraint: минимум 1000 групп в каждом сегменте
    # Objective: minimize MSE
    # Method: L-BFGS-B optimization
```

**Numerical stability:**
* Нормализация x перед fitting: `x_norm = (x - mean) / std`
* Пересчет slopes/intercepts в исходный масштаб
* Обработка edge cases (сегменты с 1 наблюдением)

**Model Selection via AICc:**
```python
# Corrected Akaike Information Criterion для finite samples
AIC = n * log(MSE) + 2*k
AICc = AIC + (2*k*(k+1)) / (n - k - 1)
```

**Балансирует:**
* Goodness of fit (минимизация MSE)
* Model complexity (penalty за количество параметров)
* **Результат:** Оптимальное число сегментов = arg min(AICc)

**Monotonicity check:**
```python
is_monotone = np.all(np.diff(y_pred) >= -1e-6)  # Success rate не должен падать
```

### 5. Local Analysis at Breakpoints

**Gaussian-weighted local mean:**
```python
# Adaptive window size: 1% данных или 30-200 групп
window_size = max(30, min(200, int(len(x) * 0.01)))

# Gaussian weights для smooth локального усреднения
weights = np.exp(-((x_local - x0)**2) / (2 * sigma**2))
local_mean = np.sum(weights * y_local)
```

**Bootstrap confidence intervals:**
* 1000 resamples для оценки 95% CI
* Weighted bootstrap с Gaussian weights
* Результат: понимание uncertainty в критических точках

### 6. Robust Threshold Detection — 4 метода

**Cross-validation подход:**
1. **Raw First-Hit** — первая группа ≥80%
2. **Smoothed First-Hit** — rolling mean (window=100) ≥80%
3. **Quantile-Based** — где P50/P60/P70 ≥80%
4. **Piecewise Model Crossing** — где fitted model пересекает 80%

**Consensus decision:** Если все методы дают похожие результаты → высокая уверенность в пороге.

### 7. Advanced Visualizations

**4-panel comprehensive view:**
```python
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

# Panel 1: Detail view (0-2500) — фокус на критическом диапазоне
# Panel 2: Log scale — весь range данных
# Panel 3: Heatmap — 2D распределение (Size × Success Rate)
# Panel 4: Bar chart — success by segment с color coding
```

**Design principles:**
* Limited y-axis (0-105%) для удаления outliers
* Color coding: Red (<60%), Orange (60-80%), Green (≥80%)
* Breakpoint annotations с вертикальными линиями
* Median values внутри bars для robustness check

**Heatmap innovation:**
* Нормализация по столбцам → % within each segment
* Adaptive segment ranges на основе breakpoints
* Text annotations только для values >2% (избежание clutter)
* Adaptive text color: white на темном фоне, black на светлом

## Воспроизводимость

**Input:** `ALL_results.parquet` (результаты Monte Carlo из Части 1)

**Output:**
* Консольный вывод: summary statistics, correlation tables, threshold analysis
* Визуализации: `threshold_analysis.png` (300 DPI, high-resolution)
* Структурированные DataFrame для дальнейшего использования

**Время выполнения:**
* Loading data: 1-2 секунды (Parquet эффективен)
* Univariate analysis: ~5 секунд
* Piecewise regression (2-7 segments testing): ~30-60 секунд
  * Optimization computationally intensive для >350K observations
* Полный анализ: ~2-3 минуты на стандартном laptop

**Критические parameters:**
* `min_segment_size=1000` — статистическая значимость каждого сегмента
* `n_bootstrap=1000` — баланс точности CI и времени выполнения
* `window_pct=0.01` — локальное сглаживание (1% данных)

---

**Технический результат:** Математически строгая валидация метода с идентификацией точных пороговых значений (141-200 employees для перехода в зону надёжности) и количественной оценкой неопределённости для business decision making.

---

## Часть 3: Генетический алгоритм - решение проблемы

### Принципиальное отличие подходов

**Метод агрегированных распределений:**
```
Предполагает → распределение по CR и рейтингам
Рассчитывает → Бюджет как математическое ожидание
Проблема → Не учитывает реальное распределение
Результат → Работает только для 200+ человек
```

**Генетический алгоритм:**
```
Использует → Реальное распределение сотрудников по CR
Оптимизирует → Матрицу под конкретную популяцию
Валидирует → Через Monte Carlo с реальными данными
Результат → Работает для малых групп
```

### Архитектура решения

**1. Генетический алгоритм**
- **Популяция:** 1000 кандидатов merit matrices
- **Эволюция:** 500 поколений с ранней остановкой
- **Операторы:** Турнирная селекция, кроссовер, мутация
- **Улучшение:** Локальная оптимизация L-BFGS-B для топ-кандидатов

**2. Многокритериальная функция fitness**

```
Fitness = 0.70 × Budget_Score + 
          0.30 × Constraint_Score + 
          Bonus × Differentiation_Score
```

где:
- **Budget Score** — попадание в ±5% от целевого бюджета (70% веса)
- **Constraint Score** — соблюдение принципов справедливости (30% веса)
- **Differentiation** — видимость различий между рейтингами (бонус)

**3. Принципы справедливости (constraints)**

Встроенные жесткие ограничения:
- ✓ Высокий рейтинг → больше повышение (монотонность)
- ✓ Высокий CR → меньше повышение (монотонность)
- ✓ Минимальные шаги дифференциации (видимая разница)
- ✓ Защита от экстремальных значений
- ✓ Anchor cells (фиксация ключевых значений)

**4. Monte Carlo валидация**

Каждая матрица тестируется на **10,000+ симуляций**:
- Perturbation целевого распределения (Dirichlet)
- Стресс-сценарии различных распределений рейтингов
- Расчет success rate: % попаданий в бюджет ±5%

### Ключевые преимущества

**1. Адаптация к реальному распределению**

Алгоритм **знает** о фактическом распределении сотрудников:
- Если 70% имеют CR = 0.95-1.05 → это учитывается
- Матрица оптимизируется под конкретную популяцию
- **Результат:** Высокая точность для малых групп (50-100 чел)

**2. Гарантия справедливости**

Все принципы fair compensation автоматически соблюдаются через встроенные constraints.

**3. Автоматическая policy guidance**

Для каждого рейтинга генерируются рекомендации:

```
Rating 1: Target 10% | Допустимо 8.2-11.8%
Rating 2: Target 15% | Допустимо 13.1-16.9%
Rating 3: Target 50% | Допустимо 48.3-51.7%
Rating 4: Target 15% | Допустимо 13.2-16.8%
Rating 5: Target 10% | Допустимо 8.3-11.7%
```

Это реалистичные границы, при которых бюджет соблюдается (на основе Monte Carlo).

### Результаты применения

**Валидация на данных исследования:**

```
Данные: 395,325 сотрудников (Москва)
Группы: Company × Function
```

**Сравнение методов:**

| Размер группы | Aggregated Distributions | Genetic Algorithm | Улучшение |
|---------------|-------------------------|-------------------|-----------|
| 50-100 | ~55% | ~85% | **+30%** |
| 100-200 | ~70% | ~92% | **+22%** |
| 200+ | ~90% | ~96% | +6% |

**Ключевой результат:** Улучшение на +22-30 процентных пунктов для групп 50-200 человек.

### Практическое применение

**Workflow:**

1. **Подготовка:** Загрузить данные сотрудников (base_salary, CR)
2. **Настройка:** Задать целевой бюджет, распределение рейтингов, constraints
3. **Оптимизация:** Запуск алгоритма (~2-5 минут)
4. **Результат:** Top-20 оптимальных матриц с fitness scores и policy guidance

**Пример выходного результата:**

```
ОПТИМАЛЬНАЯ МАТРИЦА (Success Rate: 94.2%)

                Rating 1  Rating 2  Rating 3  Rating 4  Rating 5
CR [0.0-0.80)      3.5%      6.8%     10.2%     13.5%     16.8%
CR [0.80-0.90)     2.8%      5.9%      9.1%     12.2%     15.3%
CR [0.90-1.10)     2.1%      5.0%      8.0%     11.0%     14.0%
CR [1.10-1.20)     1.4%      4.1%      6.9%      9.6%     12.3%
CR [1.20-∞)        0.7%      3.2%      5.8%      8.3%     10.8%

Metrics:
├─ Fitness: 0.9547
├─ Budget Score: 0.9823 (98.2% scenarios within ±5%)
├─ Constraint Score: 0.9871 (все принципы соблюдены)
└─ Mean Deviation: 1.23% от целевого бюджета
```

## Технические решения

## Используемые библиотеки

**Обработка данных:**
* `pandas` — загрузка employee data, группировка по CR bins
* `numpy` — векторизованные операции с матрицами, массивами зарплат

**Статистическое моделирование:**
* `scipy.stats.dirichlet` — моделирование вариаций распределения рейтингов
  * Параметр `concentration` (~167 по умолчанию) контролирует разброс вокруг целевого распределения
  * Имитирует реальность: менеджеры следуют политике HR, но с отклонениями ±2-3%
  * Метод `dirichlet.rvs()` генерирует случайные распределения, близкие к целевому
* `scipy.stats.norm` — генерация автоматических bell-curve распределений рейтингов
* `scipy.optimize.minimize` — локальная оптимизация топ-5 кандидатов
  * Метод: L-BFGS-B (bounded optimization)
  * Constraints: cell bounds, anchor cells, strict zeros

**Обработка изображений (для матриц):**
* `scipy.ndimage.gaussian_filter` — сглаживание матриц после кроссовера
  * Параметр `sigma=0.5` для мягкого устранения шума
  * Режим `mode='nearest'` для корректной обработки краёв

**Структуры данных:**
* `dataclasses` — типизированные структуры для кандидатов и policy guidance

## Ключевые технические решения

### 1. Adaptive Constraint Generation

**Merit-pool-aware constraints**
```python
def auto_configure_constraints():
    # Base step как % от merit pool, не от theoretical span
    base_step_factor = 0.15  # 15% от merit pool
    min_step_rating = merit_pool_pct * base_step_factor
    
    # Адаптивные multipliers для cell_max
    if merit_pool_pct <= 0.03:
        multiplier = 4.0  # Tight pools need higher ceiling
    elif merit_pool_pct <= 0.12:
        multiplier = 2.5
    else:
        multiplier = 1.5
```

**Обоснование:** Merit pool 8% → realistic steps ~0.8-1.2% → feasible total range ~4-5% (вместо 24%).

**Validation logic:**
```python
# Real-time feasibility check
min_range_rating = min_step_rating * (num_ratings - 1)
available_span = cell_max - cell_min

if min_range_rating > available_span * 0.9:
    print("⚠ WARNING: Rating step requirements are very tight!")
```

### 2. Dirichlet Sampling с Hard Zero Support

**Challenge:** Как моделировать вариации рейтингов, если целевое распределение содержит нули?

**Решение: Adaptive Dirichlet**
```python
def compute_dirichlet_alphas(base_probs, concentration):
    # Detect distribution peakiness via entropy
    entropy = -np.sum(base_probs * np.log(base_probs + eps))
    max_entropy = np.log(len(base_probs))
    peakiness_factor = (max_entropy - entropy) / max_entropy
    
    # Scale concentration for peaked distributions (reduce variance)
    scale = 1.0 + 1.0 * peakiness_factor
    alphas = base_probs / base_probs.sum() * concentration * scale
```

**Hard zero enforcement:**
```python
if HARD_ZERO_RATINGS and np.any(base_probs == 0):
    # Sample only from non-zero ratings
    nz_idx = np.flatnonzero(base_probs > 0)
    sub_alphas = compute_dirichlet_alphas(base_probs[nz_idx], concentration)
    sub_draw = rng.dirichlet(sub_alphas)
    
    # Reconstruct with zeros
    draw = np.zeros_like(base_probs)
    draw[nz_idx] = sub_draw
```

**Эффект:** Рейтинги с 0% target полностью исключены из Monte Carlo (не появятся даже с epsilon вероятностью).

### 3. Векторизованная Fitness Evaluation

**Критическая оптимизация:** Оценка fitness — самая частая операция (50K+ вызовов).

**Pre-stacking scenarios:**
```python
# ВМЕСТО: цикл по scenarios в каждом evaluate_fitness call
for scenario in scenarios:
    cost = (scenario * matrix).sum()
    
# ИСПОЛЬЗУЕМ: векторизация через broadcasting
S_stack = np.stack(scenarios)  # Shape: [K scenarios, I bins, J ratings]
costs = (S_stack * matrix).sum(axis=(1, 2))  # Shape: [K]
```

**Асимметричная budget tolerance:**
```python
# Классический подход: symmetric ±5%
within_budget = (np.abs(deviations) <= tolerance).sum()

# Новый подход: asymmetric -5% to +1.5%
within_budget = (
    (deviations >= -BUDGET_TOLERANCE_LOWER) & 
    (deviations <= BUDGET_TOLERANCE_UPPER)
).sum()
```

**Обоснование:** Underspend (-5%) менее критичен для бизнеса, чем overspend (+1.5%).

**Band-aware penalty:**
```python
def _band_penalty(deviation, lower, upper):
    if deviation < -lower:
        return (-deviation - lower) / lower  # Scaled underspend penalty
    if deviation > upper:
        return (deviation - upper) / upper   # Scaled overspend penalty
    return 0.0  # Inside band → zero penalty
```

### 4. Differentiation Score

**Мотивация:** Матрицы с минимальными шагами удовлетворяют constraints, но не дают видимой дифференциации.

**Robust slope measurement:**
```python
def differentiation_score(matrix):
    for i in range(num_cr_bins):
        steps = []
        for j in range(num_ratings - 1):
            if not (STRICT_ZERO_MASK[i, j] or STRICT_ZERO_MASK[i, j+1]):
                steps.append(max(matrix[i, j+1] - matrix[i, j], 0.0))
        
        mean_step = np.mean(steps)
        
        # Target: 15% выше minimum, Cap: 50% от maximum
        target = CONSTRAINTS['min_step_rating'] * 1.15
        cap = CONSTRAINTS['step_max_rating'] * 0.5
        
        score = min(mean_step, cap) / target  # Normalized 0..1
```

**Row weighting:** Нижние CR bins (core population) получают больший вес:
```python
weights = np.linspace(1.0, 0.6, num_cr_bins)  # Row 0: 1.0, Last row: 0.6
final_score = np.average(per_row_scores, weights=weights)
```

**Integration в fitness:**
```python
total_fitness = (
    0.70 * budget_score +      # Primary: budget accuracy
    0.30 * constraint_score +  # Secondary: constraint satisfaction
    DIFF_WEIGHT * diff_score   # Bonus: visible differentiation
)
```

**Типичный DIFF_WEIGHT:** 0.10-0.30 (достаточно для "nudge", не доминирует над budget).

### 5. Генетический Алгоритм — Architecture

**Population initialization: Structured seeding**
```python
# 70% structured matrices (monotonic, step-aware)
matrix = create_structured_matrix()  

# 30% with aggressive zeros (if cell_min = 0)
matrix = create_structured_matrix_with_zeros()
```

**Selection: Tournament**
```python
# Выбор k=5 случайных, возврат best
def tournament_selection(population, k=5):
    tournament = random.sample(population, k)
    return max(tournament, key=lambda x: x.fitness)
```

**Crossover: Blend with smoothing**
```python
def crossover(parent1, parent2):
    alpha = uniform(0.3, 0.7)  # Controlled blend
    child = alpha * parent1 + (1 - alpha) * parent2
    child = gaussian_filter(child, sigma=0.5)  # Smooth noise
    return child
```

**Mutation: Structure-aware perturbation**
```python
def mutate(matrix, rate=0.25):
    for i, j in matrix.indices:
        if random() < rate:
            delta = normal(0, cell_max * 0.05)  # 5% of max
            matrix[i, j] += delta
    return repair_matrix(matrix)  # Fix monotonicity
```

**Elitism:** Top 10% (100/1000) переходят без изменений → сохранение best solutions.

**Early stopping:**
```python
if improvement < 1e-5 for 50 generations:
    break  # Converged
```

### 6. Matrix Repair — Monotonicity Enforcement

**Проблема:** После мутации/кроссовера матрица может нарушить монотонность.

**Rating monotonicity (horizontal):**
```python
for i in range(num_cr_bins):
    for j in range(1, num_ratings):
        if matrix[i, j] < matrix[i, j-1]:
            # Force: rating j >= rating j-1
            matrix[i, j] = matrix[i, j-1] + min_step_rating * 0.5
```

**CR monotonicity (vertical):**
```python
for i in range(1, num_cr_bins):
    for j in range(num_ratings):
        if matrix[i, j] > matrix[i-1, j]:
            # Force: CR bin i <= CR bin i-1
            matrix[i, j] = max(cell_min, matrix[i-1, j] - min_step_cr * 0.5)
```

**Skip forced zeros:** Monotonicity checks игнорируют strict zero cells.

### 7. Anchor Cells & Strict Zeros

**Anchor cells: Exact pinning**
```python
anchors = {
    'max': (0, num_ratings-1, ANCHOR_CELL_MAX),  # Best performers
    'min': (num_cr_bins-1, 0, ANCHOR_CELL_MIN)   # Worst performers
}

# Applied at every modification
matrix[i_max, j_max] = ANCHOR_CELL_MAX
matrix[i_min, j_min] = ANCHOR_CELL_MIN
```

**Strict zeros: Multiple cells**
```python
FORCE_ZERO_CELLS = {
    4: [0, 1],  # CR bin 4, ratings 1-2 → 0%
    3: [0]      # CR bin 3, rating 1 → 0%
}

# Massive penalty for violations
if STRICT_ZERO_MASK[i, j] and abs(matrix[i, j]) > eps:
    penalties.append(abs(matrix[i, j]) * 1000)
```

### 8. Local Refinement (Top 5 Only)

**После GA: L-BFGS-B optimization для топ-5 кандидатов**
```python
def local_refinement(matrix, S_stack, merit_pool):
    def objective(x):
        mat = x.reshape(num_cr_bins, num_ratings)
        mat = repair_matrix(mat)
        fitness, _, _, _, _ = evaluate_fitness(mat, S_stack, merit_pool)
        return -fitness  # Minimize negative fitness
    
    bounds = [(cell_min, cell_max)] * matrix.size
    
    # Override bounds for fixed cells
    for i, j in anchor_positions:
        bounds[idx] = (exact_value, exact_value)
    for i, j in strict_zero_positions:
        bounds[idx] = (0.0, 0.0)
    
    result = minimize(objective, matrix.flatten(), 
                     method='L-BFGS-B', bounds=bounds, 
                     options={'maxiter': 100})
```

**Rationale:** GA находит basin of attraction, L-BFGS-B fine-tunes до локального оптимума.

### 9. Stress Testing & Policy Guidance

**Stress scenarios: 5 типов распределений**
```python
scenarios = {
    'target': TARGET_RATING_DISTRIBUTION,  # Baseline
    'inflated': skew_toward_high_ratings,  # Grade inflation
    'harsh': skew_toward_low_ratings,      # Strict managers
    'forced_curve': bell_curve,            # Forced ranking
    'top_heavy': 70% in upper half         # Leniency
}
```

**Policy generation: Percentile-based ranges**
```python
# Run 200 successful scenarios
for _ in range(200):
    if budget_within_tolerance:
        successful_dists.append(rating_distribution)

# For each rating: P10-P90 range
PolicyGuidance(
    rating=rating,
    target_pct=TARGET[rating],
    hard_min_pct=np.percentile(successful_dists[rating], 10),
    hard_max_pct=np.percentile(successful_dists[rating], 90)
)
```

**Output:** "Для 80% success rate держите рейтинг 5 в диапазоне 8-12% (target 10%)".

### 10. Deduplication & Export

**Problem:** Crossover может создавать дубликаты.

**Solution: Matrix-level deduplication**
```python
def deduplicate_candidates(candidates, tolerance=1e-6):
    unique = []
    for candidate in candidates:
        if not any(np.allclose(candidate.matrix, seen, atol=tolerance) 
                   for seen in unique):
            unique.append(candidate)
    return unique
```

**Excel export: Multi-sheet workbook**
```
Rank_01:
  - Metadata (fitness, success rate, budget score, constraint score, diff score)
  - Merit Matrix (% format, CR bins × Ratings)
  - Policy Guidance (target/min/max % для каждого рейтинга)
  
Rank_02:
  ...
  
Rank_20:
  ...
```

## Воспроизводимость

**Input:** 
* `Company_Data.xlsx` (employee data: base_salary, CR) или synthetic data (n=1200, lognormal salaries)
* Configuration: merit pool, tolerances, constraints, anchor cells

**Output:**
* `artifacts/merit_matrices_all_scenarios.xlsx` — 20 sheets с топ-20 матрицами
* `artifacts/candidate_summary.csv` — ранжирование по fitness
* `artifacts/optimization_config.json` — полная конфигурация запуска

**Seeds для воспроизводимости:**
* `SEED_BASE_POPULATION = 42` — synthetic employee data
* `SEED_SCENARIOS = 2025` — Monte Carlo scenario generation
* `SEED_GA = 89` — genetic algorithm initialization

**Время выполнения:**
* Population initialization (1000 matrices): ~5-10 секунд
* GA evolution (500 generations, 1000 pop): ~15-30 минут
  * Каждое поколение: 1000 fitness evaluations × 2K scenarios
  * С quick_eval_scenarios=2000, full_eval_scenarios=10000
* Local refinement (top 5): ~2-5 минут
* Полный запуск: **~20-40 минут** на стандартном laptop (8 cores, 16GB RAM)

**Scaling considerations:**
```python
if len(employees) > 5000 and total_evals > 60_000:
    print("Large run detected. Consider reducing:")
    print("  - population_size (1000 → 500)")
    print("  - num_generations (500 → 250)")
```

---

**Технический результат:** Генетический алгоритм с адаптивными constraints, асимметричной budget tolerance и дифференциацией находит merit matrices, которые:
1. **Попадают в бюджет** в 70-95% Monte Carlo сценариев (vs ~40-50% у наивных подходов)
2. **Обеспечивают видимую дифференциацию** через differentiation score
3. **Удовлетворяют HR constraints** (monotonicity, step sizes, anchor cells)
4. **Устойчивы к вариациям** распределения рейтингов (stress tests)

**Практическое применение:** Компании с merit pool 8-13% и группами 100-1000 сотрудников получают matrices, которые балансируют budget discipline и performance differentiation без ручной итерации.

---

## Общие выводы

### Трехэтапный проект

**Этап 1: Валидация (Monte Carlo)**
- **Данные:** Реальные рыночные данные по состоянию на 1 мая 2025 года
  - 300+ компаний в Москве
  - Полная функциональная вертикаль (от самых младшых должностей до топ-менеджмента)
  - 2,921 функциональная группа (Company x Function)
  - 383,000+ сотрудников
- **Методология:** Monte Carlo симуляция (50,000 итераций на группу)
- **Результат:** Метод математически корректен, но требует 300-400+ сотрудников для надежности. Выявлена "зона опасной иллюзии" (141-299 чел), где средние SR 81-88% маскируют 22-46% риск провала.

**Этап 2: Детальный анализ**
- Размер группы объясняет ~85-90% вариации в надежности
- Конфигурация матрицы влияет минимально (±6%)
- **Парадокс высоких средних:** средняя SR=88% не гарантирует надежность из-за бимодального распределения
- **Фундаментальное противоречие:** статистика требует 300-400+ человек, но <20% функций достигают этого порога. Метод непригоден для 70-75% бизнес-контекста.

**Этап 3: Решение (Генетический алгоритм)**
- Работает для групп от 50 человек
- Улучшение на +22-30% для малых/средних групп
- Применим для 70-80% бизнес-контекстов, где традиционный метод ненадежен

---

### Практическая ценность

**Для HR:**
- Понимание критических ограничений: требуется 300+ человек, "зона иллюзии" 141-299 чел опасна
- Генетический алгоритм решает проблему 70-80% функций, где традиционный метод неприменим
- Количественная оценка рисков по каждой функции
- Автоматизация создания справедливых merit matrices

**Для Finance:**
- Количественная оценка бюджетных рисков по сегментам (от 0% до 100% риска выхода за бюджет)
- Предсказуемость: известна вероятность попадания в бюджет ±5% для каждой функции
- Data-driven аргументация для CFO и совета директоров

**Для бизнеса:**
- Расширение применимости с 15-20% (только крупнейшие функции) до 70-80% (включая малые)
- Снижение риска перерасхода на 50-70% для малых/средних функций
- Гарантия справедливости через автоматические constraints

---

### Научный вклад

1. **Первое систематическое исследование** точности aggregated distributions method на реальных рыночных данных (383K+ сотрудников, 2,921 группа, 1.7+ миллиардов симуляций)

2. **Выявление критических порогов и "зоны иллюзии":**
   - Количественное определение границ применимости (300-400+ чел)
   - Открытие парадокса высоких средних (почему средняя SR=88% обманчива)

3. **Количественная оценка практической неприменимости:** только 15-20% бизнес-контекстов могут надежно использовать традиционный метод

4. **Разработка практического решения:** генетический алгоритм расширяет применимость до 70-80% контекстов с улучшением на +22-30%

5. **Мост между теорией и практикой:** коммерческое применение академических методов оптимизации для решения реальной HR/Finance проблемы

---

### Ключевые рекомендации

**Универсальное правило:**
- **<300 человек** → обязательна оптимизация
- **300-399 человек** → желательна оптимизация (92% vs 98%+ надежности)
- **400+ человек** → традиционный метод надежен

**По размеру компаний:**
- **<1,000 чел:** Традиционный метод неприменим для большинства функций
- **1,000-10,000 чел:** Применим для 10-30% крупнейших функций, остальным нужна оптимизация
- **10,000+ чел:** Применим для 30-40% функций, остальным желательна оптимизация

**Критическое предупреждение:** Остерегайтесь "зоны иллюзии" (141-299 чел) — высокие средние SR маскируют реальный риск провала в 22-46% случаев.

---

## Статус проекта

**Академическое исследование** (Этапы 1-2): Завершено
- Валидация метода на данных 383K+ сотрудников
- Анализ результатов и выявление ограничений
- Публикация результатов (данный документ)

**Коммерческое решение** (Этап 3):
- Генетический алгоритм разработан и валидирован
- Доступен исключительно клиентам Lens Consulting
- Применяется в консалтинговых проектах по C&B
